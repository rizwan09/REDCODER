---
language: sanskrit
---

# RoBERTa trained on Sanskrit (SanBERTa)

**Mode size** (after training): **340MB**

### Dataset:

[Wikipedia articles](https://www.kaggle.com/disisbig/sanskrit-wikipedia-articles) (used in [iNLTK](https://github.com/goru001/nlp-for-sanskrit)).
It contains evaluation set.

[Sanskrit scraps from CLTK](http://cltk.org/)

### Configuration

| Parameter | Value |
|---|---|
| `num_attention_heads` | 12 |
| `num_hidden_layers` | 6 |
| `hidden_size` | 768 |
| `vocab_size` | 29407 |

### Training :
- On TPU
- For language modelling
- Iteratively increasing `--block_size` from 128 to 256 over epochs

### Evaluation

|Metric| # Value |
|---|---|
|Perplexity (`block_size=256`)|4.04|

## Example of usage:

### For Embeddings

```

tokenizer = AutoTokenizer.from_pretrained("surajp/SanBERTa")
model = RobertaModel.from_pretrained("surajp/SanBERTa")

op = tokenizer.encode("рдЗрдпрдВ рднрд╛рд╖рд╛ рди рдХреЗрд╡рд▓рдВ рднрд╛рд░рддрд╕реНрдп рдЕрдкрд┐ рддреБ рд╡рд┐рд╢реНрд╡рд╕реНрдп рдкреНрд░рд╛рдЪреАрдирддрдорд╛ рднрд╛рд╖рд╛ рдЗрддрд┐ рдордиреНрдпрддреЗред", return_tensors="pt")
ps = model(op)
ps[0].shape

```
```
'''
Output:
--------
torch.Size([1, 47, 768])

```


### For \<mask\> Prediction

```
from transformers import pipeline

fill_mask = pipeline(
    "fill-mask",
    model="surajp/SanBERTa",
    tokenizer="surajp/SanBERTa"
)

## рдЗрдпрдВ рднрд╛рд╖рд╛ рди рдХреЗрд╡рд▓рдВ рднрд╛рд░рддрд╕реНрдп рдЕрдкрд┐ рддреБ рд╡рд┐рд╢реНрд╡рд╕реНрдп рдкреНрд░рд╛рдЪреАрдирддрдорд╛ рднрд╛рд╖рд╛ рдЗрддрд┐ рдордиреНрдпрддреЗред
fill_mask("рдЗрдпрдВ рднрд╛рд╖рд╛ рди рдХреЗрд╡рд▓<mask> рднрд╛рд░рддрд╕реНрдп рдЕрдкрд┐ рддреБ рд╡рд┐рд╢реНрд╡рд╕реНрдп рдкреНрд░рд╛рдЪреАрдирддрдорд╛ рднрд╛рд╖рд╛ рдЗрддрд┐ рдордиреНрдпрддреЗред")

ps = model(torch.tensor(enc).unsqueeze(1))
print(ps[0].shape)
```
```
'''
Output:
--------
[{'score': 0.7516744136810303,
  'sequence': '<s> рдЗрдпрдВ рднрд╛рд╖рд╛ рди рдХреЗрд╡рд▓рдВ рднрд╛рд░рддрд╕реНрдп рдЕрдкрд┐ рддреБ рд╡рд┐рд╢реНрд╡рд╕реНрдп рдкреНрд░рд╛рдЪреАрдирддрдорд╛ рднрд╛рд╖рд╛ рдЗрддрд┐ рдордиреНрдпрддреЗред</s>',
  'token': 280,
  'token_str': '├а┬д─д'},
 {'score': 0.06230105459690094,
  'sequence': '<s> рдЗрдпрдВ рднрд╛рд╖рд╛ рди рдХреЗрд╡рд▓реА рднрд╛рд░рддрд╕реНрдп рдЕрдкрд┐ рддреБ рд╡рд┐рд╢реНрд╡рд╕реНрдп рдкреНрд░рд╛рдЪреАрдирддрдорд╛ рднрд╛рд╖рд╛ рдЗрддрд┐ рдордиреНрдпрддреЗред</s>',
  'token': 289,
  'token_str': '├а┬е─в'},
 {'score': 0.055410224944353104,
  'sequence': '<s> рдЗрдпрдВ рднрд╛рд╖рд╛ рди рдХреЗрд╡рд▓рд╛ рднрд╛рд░рддрд╕реНрдп рдЕрдкрд┐ рддреБ рд╡рд┐рд╢реНрд╡рд╕реНрдп рдкреНрд░рд╛рдЪреАрдирддрдорд╛ рднрд╛рд╖рд╛ рдЗрддрд┐ рдордиреНрдпрддреЗред</s>',
  'token': 265,
  'token_str': '├а┬д┬╛'},
  ...]
```

### It works!! ЁЯОЙ ЁЯОЙ ЁЯОЙ

> Created by [Suraj Parmar/@parmarsuraj99](https://twitter.com/parmarsuraj99) | [LinkedIn](https://www.linkedin.com/in/parmarsuraj99/)

> Made with <span style="color: #e25555;">&hearts;</span> in India
